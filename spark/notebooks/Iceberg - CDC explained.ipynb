{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1041ae6f",
   "metadata": {},
   "source": [
    "![iceberg-logo](https://www.apache.org/logos/res/iceberg/iceberg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247fb2ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### [CDC, Spark, and Iceberg: The Resilinet Way to Lakehouse!](https://tabular.io/blog/docker-spark-and-iceberg/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a5c8206",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/20 14:15:52 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://981a1bf21baf:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff86832af0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Jupyter\").getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99ec545-048b-4f6e-9f45-51b72f40d33d",
   "metadata": {},
   "source": [
    "### CDC data\n",
    "\n",
    "For this notebook, we will use a custom data generator to showcase the problems that happens while mirroring source tables in data lakes. This function creates data for a sample dimension table for a data product company."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aeae22-c54f-4f8e-9ea2-02e6aca40f25",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Installing dependencies \n",
    "faker is a library that generates random data for such usecases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bd8475d-9378-47e6-9f39-70413ff22549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faker in /usr/local/lib/python3.9/site-packages (26.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.9/site-packages (from faker) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install faker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f3936b-d335-4359-8399-2abc59cc33da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Imports and constants \n",
    "\n",
    "Paths are defined here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c9915ba-f481-4039-933e-a9dfd3738008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.functions import col, lit, current_timestamp, expr\n",
    "from datetime import datetime as dt\n",
    "from faker import Faker\n",
    "from uuid import uuid1\n",
    "\n",
    "fake = Faker()\n",
    "GLOBAL_TEST_PATH = 'tests/test_data'\n",
    "DATA_PATHS = {\n",
    "    'DMS': {\n",
    "        'SOURCE': f'{GLOBAL_TEST_PATH}/db_data',\n",
    "        'SINK': f'{GLOBAL_TEST_PATH}/dms_sink'\n",
    "    }\n",
    "}\n",
    "TABLES = {\n",
    "    1: {\n",
    "        'name': 'user_dimension',\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb76225-c002-479d-a488-1a0860be7084",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Helper functions\n",
    "\n",
    "These functions are going to help up create data in parquet and load them in iceberg\n",
    "\n",
    "1. create_mock_dimension_table_dataframe\n",
    "2. get_location\n",
    "3. extract_parquet\n",
    "4. load_parquet\n",
    "5. load_table\n",
    "6. load_mock_db_initial_state\n",
    "7. load_mock_dms_full_load\n",
    "8. load_mock_dms_cdc_load\n",
    "9. load_mock_dms_cdc_d_load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "009e9b07-14a0-436e-85c9-6c53b94fd731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mock_dimension_table_dataframe(spark, row_count=10):\n",
    "    \"\"\" Create initial test data for dimension table\n",
    "    This function creates both pre- and post- transformation data\n",
    "    saved as Parquet files in tests/test_data. This will be used for\n",
    "    unit tests as well as to load as a part of example ingestion job\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    ts = dt.now()\n",
    "    df = spark.range(row_count)\n",
    "    local_records = [\n",
    "        Row(\n",
    "            name=fake.name(),\n",
    "            address=fake.address(),\n",
    "            license_number=fake.license_plate(),\n",
    "            iban=fake.iban(),\n",
    "            bs=fake.bs(),\n",
    "            catch_phrase=fake.catch_phrase(),\n",
    "            company=fake.company(),\n",
    "            paragraph=fake.paragraph(nb_sentences=10),\n",
    "            created_at=fake.date_time(),\n",
    "            updated_at=ts\n",
    "        )\n",
    "        for i in range(row_count)\n",
    "    ]\n",
    "    df = spark.createDataFrame(local_records)\n",
    "    df = df.withColumn(\"uuid\", expr(\"uuid()\"))\n",
    "\n",
    "    print(f\"\"\"\n",
    "        dataframe of count {row_count} \n",
    "        was created from faker as {ts}\n",
    "    \"\"\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efb5c509-bc62-4925-be6b-ddb97589016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location(actor, io, table_name, action):\n",
    "    \"\"\" Get Location\n",
    "    <actors>/<table_name>/<action>/<timestamp>\n",
    "\n",
    "    :params actor: executor that called the function\n",
    "    :params io: intended SOURCE or SINK\n",
    "    :params table_name: name of that table that the location\n",
    "        is associated to\n",
    "    :params action: key event that caused the data creation\n",
    "    :return: Location\n",
    "    \"\"\"\n",
    "    ts_str = dt.now().strftime(\"%Y_%m_%d-%I:%M:%S_%p\")\n",
    "    loc = f'{DATA_PATHS[actor][io]}/{table_name}/{action}/{ts_str}'\n",
    "    print(f'''\n",
    "        Location: {loc}\n",
    "            for {actor}\n",
    "            on {io} \n",
    "            for {table_name} \n",
    "            on {action}\n",
    "            at {ts_str}\n",
    "    ''')\n",
    "    return loc\n",
    "\n",
    "def extract_parquet(spark, loc):\n",
    "    \"\"\"Load data from Parquet file format.\n",
    "\n",
    "    :param spark: Spark session object.\n",
    "    :param loc: Location of data\n",
    "    :return: Spark DataFrame.\n",
    "    \"\"\"\n",
    "    df = (\n",
    "        spark\n",
    "        .read\n",
    "        .parquet(loc))\n",
    "    print(f\"\"\"\n",
    "        dataframe of count {df.count()} '\n",
    "        was read from location {loc}'\n",
    "    \"\"\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_parquet(df, loc, num_of_output_files=1):\n",
    "    \"\"\"Load data to Parquet file format.\n",
    "\n",
    "    :param df: Spark Dataframe\n",
    "    :param loc: Location of data\n",
    "    :param num_of_output_files: number of files\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    (df\n",
    "     .coalesce(num_of_output_files)\n",
    "     .write\n",
    "     .parquet(loc, mode='overwrite')\n",
    "     )\n",
    "\n",
    "    print(f'''\n",
    "        Successfully written {num_of_output_files} file\n",
    "        for on location {loc}\n",
    "    ''')\n",
    "\n",
    "\n",
    "def load_table(df, database_name, table_name):\n",
    "    \"\"\" Write a table in associated catalog\n",
    "\n",
    "    :params df: Dataframe to write\n",
    "    :params database_name: namespace as in catalog\n",
    "    :params table_name: table's name for reference\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    df.write.saveAsTable(f\"{database_name}.{table_name}\")\n",
    "    print(f' Successfully added table {database_name}.{table_name} to catalog ')\n",
    "\n",
    "def load_mock_db_initial_state(spark, df, table_name):\n",
    "    \"\"\" Write mock initial mock data for dimension table\n",
    "    This function creates mock initial load data that DMS would have\n",
    "    generated for a dimensional table that stores names of all entities\n",
    "    saved as Parquet files in tests/test_data. This will be used for\n",
    "    unit tests as well as to load as a part of example ingestion job.\n",
    "    :return: Location\n",
    "    \"\"\"\n",
    "\n",
    "    location = get_location(\n",
    "        actor='DMS',\n",
    "        io='SOURCE',\n",
    "        table_name=table_name,\n",
    "        action='initial_state'\n",
    "    )\n",
    "    load_parquet(\n",
    "        df=df,\n",
    "        loc=location\n",
    "    )\n",
    "\n",
    "    print(f'''\n",
    "        Successfully written Mock DB initial state\n",
    "        for table {table_name} on location {location}\n",
    "    ''')\n",
    "    return location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6cfbcad-7985-464a-9934-b0a0064a5169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mock_dms_full_load(table_name, df):\n",
    "    \"\"\" Mock AWS DMS full load\n",
    "    :param table_name: Source Table Name\n",
    "    :param df: Source Table Data\n",
    "    \"\"\"\n",
    "    df_initial_load = (\n",
    "        df\n",
    "        .withColumn(\n",
    "            \"Op\",\n",
    "            lit('I')\n",
    "        )\n",
    "        .withColumn(\n",
    "            'dms_export_timestamp',\n",
    "            current_timestamp()\n",
    "        )\n",
    "    )\n",
    "    location = get_location(\n",
    "        actor='DMS',\n",
    "        io='SINK',\n",
    "        table_name=table_name,\n",
    "        action='full_load'\n",
    "    )\n",
    "    load_parquet(\n",
    "        df=df_initial_load,\n",
    "        loc=location\n",
    "    )\n",
    "    print(f'''\n",
    "        Successfully written Mock DMS full initial load\n",
    "        for table {table_name} on location {location}\n",
    "    ''')\n",
    "\n",
    "    return location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "631b60a2-da8e-4be7-8a99-9a1aedad9cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mock_dms_cdc_load(table_name, df):\n",
    "    \"\"\" Mock AWS DMS full load\n",
    "    :param table_name: Source Table Name\n",
    "    :param df: Source Table Data\n",
    "    \"\"\"\n",
    "    df_initial_load = (\n",
    "        df\n",
    "        .withColumn(\n",
    "            \"Op\",\n",
    "            lit('U')\n",
    "        )\n",
    "        .withColumn(\n",
    "            'dms_export_timestamp',\n",
    "            current_timestamp()\n",
    "        )\n",
    "    )\n",
    "    location = get_location(\n",
    "        actor='DMS',\n",
    "        io='SINK',\n",
    "        table_name=table_name,\n",
    "        action='cdc_load'\n",
    "    )\n",
    "    load_parquet(\n",
    "        df=df_initial_load,\n",
    "        loc=location\n",
    "    )\n",
    "    print(f'''\n",
    "        Successfully written Mock DMS full initial load\n",
    "        for table {table_name} on location {location}\n",
    "    ''')\n",
    "\n",
    "    return location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fee45bf4-5b6b-4e16-863c-0653286e5966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mock_dms_cdc_d_load(table_name, df):\n",
    "    \"\"\" Mock AWS DMS full load\n",
    "    :param table_name: Source Table Name\n",
    "    :param df: Source Table Data\n",
    "    \"\"\"\n",
    "    df_initial_load = (\n",
    "        df\n",
    "        .withColumn(\n",
    "            \"Op\",\n",
    "            lit('D')\n",
    "        )\n",
    "        .withColumn(\n",
    "            'dms_export_timestamp',\n",
    "            current_timestamp()\n",
    "        )\n",
    "    )\n",
    "    location = get_location(\n",
    "        actor='DMS',\n",
    "        io='SINK',\n",
    "        table_name=table_name,\n",
    "        action='cdc_load'\n",
    "    )\n",
    "    load_parquet(\n",
    "        df=df_initial_load,\n",
    "        loc=location\n",
    "    )\n",
    "    print(f'''\n",
    "        Successfully written Mock DMS full initial load\n",
    "        for table {table_name} on location {location}\n",
    "    ''')\n",
    "\n",
    "    return location\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19252dd6-2b41-4485-b7de-52329bd214dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Creating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86354528-ea9a-4a10-855b-d1b66fbf84c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        dataframe of count 1000 \n",
      "        was created from faker as 2024-07-15 11:07:44.444560\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "df_dimension_table = create_mock_dimension_table_dataframe(\n",
    "    spark,\n",
    "    1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00bb9829-d668-4356-a546-878d8754512e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " name           | Sean Carrillo                                                                                                                                                                                                                                                     \n",
      " address        | 53169 Jeffrey Branch Suite 941\\nChavezville, MS 81411                                                                                                                                                                                                             \n",
      " license_number | 547-YPVP                                                                                                                                                                                                                                                          \n",
      " iban           | GB27WTPF95542550296667                                                                                                                                                                                                                                            \n",
      " bs             | optimize viral communities                                                                                                                                                                                                                                        \n",
      " catch_phrase   | Multi-channeled even-keeled extranet                                                                                                                                                                                                                              \n",
      " company        | Hubbard-Wright                                                                                                                                                                                                                                                    \n",
      " paragraph      | Field political building popular range. These cup particularly process. Type friend decision move forward teach. Cold total around much compare. Unit dinner even seem. Beyond official owner white like former task something. Discover participant laugh least. \n",
      " created_at     | 1996-05-28 09:51:33.791329                                                                                                                                                                                                                                        \n",
      " updated_at     | 2024-07-15 11:07:44.44456                                                                                                                                                                                                                                         \n",
      " uuid           | 2d762923-2cb9-4489-b8d8-23d18ad1aa8f                                                                                                                                                                                                                              \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dimension_table.show(1, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3120b7-5561-4077-b950-2a8e6fa6dbc8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Writing data as parquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cfb7bd1-612a-4bc0-a302-38d1215a8cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Location: tests/test_data/db_data/demo/initial_state/2024_07_15-11:08:18_AM\n",
      "            for DMS\n",
      "            on SOURCE \n",
      "            for demo \n",
      "            on initial_state\n",
      "            at 2024_07_15-11:08:18_AM\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Successfully written 1 file\n",
      "        for on location tests/test_data/db_data/demo/initial_state/2024_07_15-11:08:18_AM\n",
      "    \n",
      "\n",
      "        Successfully written Mock DB initial state\n",
      "        for table demo on location tests/test_data/db_data/demo/initial_state/2024_07_15-11:08:18_AM\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "loc_db_initial_state = load_mock_db_initial_state(\n",
    "    spark,\n",
    "    df_dimension_table,\n",
    "    TABLES[1]['name']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29962e03-010e-4321-8fe3-62143fd559f7",
   "metadata": {},
   "source": [
    "### Mocking Database Migration Services output as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1adb86d-f273-4ba3-8576-01ea3f4555e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        dataframe of count 1000 '\n",
      "        was read from location tests/test_data/db_data/demo/initial_state/2024_07_15-11:08:18_AM'\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "df_mock_db_initial_state = extract_parquet(spark, loc_db_initial_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e052c46-bc12-4d66-a739-a1e3c95a62b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Full load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ec8f88c-446e-4a8f-8d96-826696848ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Location: tests/test_data/dms_sink/demo/full_load/2024_07_15-01:01:01_PM\n",
      "            for DMS\n",
      "            on SINK \n",
      "            for demo \n",
      "            on full_load\n",
      "            at 2024_07_15-01:01:01_PM\n",
      "    \n",
      "\n",
      "        Successfully written 1 file\n",
      "        for on location tests/test_data/dms_sink/demo/full_load/2024_07_15-01:01:01_PM\n",
      "    \n",
      "\n",
      "        Successfully written Mock DMS full initial load\n",
      "        for table demo on location tests/test_data/dms_sink/demo/full_load/2024_07_15-01:01:01_PM\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "loc_mock_dms_data = load_mock_dms_full_load(\n",
    "    TABLES[1]['name'],\n",
    "    df_mock_db_initial_state\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657ac353-efa4-4561-9572-c338f2f4ff58",
   "metadata": {},
   "source": [
    "#### CDC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829db907-f355-40ee-a811-c9600c7d8ae2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### mock updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5237b575-1e1f-47ca-b9f7-cd26b546439b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Location: tests/test_data/dms_sink/demo/cdc_load/2024_07_15-01:25:44_PM\n",
      "            for DMS\n",
      "            on SINK \n",
      "            for demo \n",
      "            on cdc_load\n",
      "            at 2024_07_15-01:25:44_PM\n",
      "    \n",
      "\n",
      "        Successfully written 1 file\n",
      "        for on location tests/test_data/dms_sink/demo/cdc_load/2024_07_15-01:25:44_PM\n",
      "    \n",
      "\n",
      "        Successfully written Mock DMS full initial load\n",
      "        for table demo on location tests/test_data/dms_sink/demo/cdc_load/2024_07_15-01:25:44_PM\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "loc_mock_dms_data_u = load_mock_dms_cdc_load(\n",
    "    TABLES[1]['name'],\n",
    "    df_mock_db_initial_state\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75212ed-4a9a-41d7-819c-b3be110e6742",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### mock deletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "adf41e22-7c7c-4c44-bb0b-afe910bd4510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Location: tests/test_data/dms_sink/demo/cdc_load/2024_07_15-01:01:05_PM\n",
      "            for DMS\n",
      "            on SINK \n",
      "            for demo \n",
      "            on cdc_load\n",
      "            at 2024_07_15-01:01:05_PM\n",
      "    \n",
      "\n",
      "        Successfully written 1 file\n",
      "        for on location tests/test_data/dms_sink/demo/cdc_load/2024_07_15-01:01:05_PM\n",
      "    \n",
      "\n",
      "        Successfully written Mock DMS full initial load\n",
      "        for table demo on location tests/test_data/dms_sink/demo/cdc_load/2024_07_15-01:01:05_PM\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "loc_mock_dms_data_d = load_mock_dms_cdc_d_load(\n",
    "    TABLES[1]['name'],\n",
    "    df_mock_db_initial_state\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77598f6d-efdb-46cb-b6c4-78ca1f54bdbf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### mock extracting data from parquet to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf45e31e-63a5-4da5-b25e-7bbfbc860a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        dataframe of count 1000 '\n",
      "        was read from location tests/test_data/dms_sink/demo/full_load/2024_07_15-01:01:01_PM'\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "df_mock_dms_data = extract_parquet(spark, loc_mock_dms_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "300993fe-cd84-496d-90f0-1a1f008d1130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        dataframe of count 1000 '\n",
      "        was read from location tests/test_data/dms_sink/demo/cdc_load/2024_07_15-01:01:05_PM'\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "df_mock_dms_data_d = extract_parquet(spark, loc_mock_dms_data_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c43ffd04-e777-441a-8214-ed174ef048a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        dataframe of count 1000 '\n",
      "        was read from location tests/test_data/dms_sink/demo/cdc_load/2024_07_15-01:25:44_PM'\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "df_mock_dms_data_u = extract_parquet(spark, loc_mock_dms_data_u)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb188745-9a14-4f95-9e45-256e0bc57bbc",
   "metadata": {},
   "source": [
    "### Creating tables in data lakehouse "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16ddb6e-65f0-4559-8401-f5f0acc2377b",
   "metadata": {},
   "source": [
    "To be able to rerun the notebook several times, let's drop the table if it exists to start fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "930682ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE DATABASE IF NOT EXISTS prod_mirror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f918310a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "DROP TABLE IF EXISTS prod_mirror.user_dimension "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9451b62-54aa-4875-8106-39ba739073cd",
   "metadata": {},
   "source": [
    "##### Loading data from dataframe to minio data lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bc0d56dc-be16-4cf0-9f77-fa96f380b3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Successfully added table prod_mirror.user_dimension to catalog \n"
     ]
    }
   ],
   "source": [
    "load_table(df_mock_dms_data, 'prod_mirror', 'user_dimension')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bcf99fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>cnt</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1000</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+------+\n",
       "|  cnt |\n",
       "+------+\n",
       "| 1000 |\n",
       "+------+"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT COUNT(*) as cnt\n",
    "FROM prod_mirror.user_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f3471ed-28d9-4e96-a497-d6d5a1d5124e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>cnt</th>\n",
       "            <th>op</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1000</td>\n",
       "            <td>I</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+------+----+\n",
       "|  cnt | op |\n",
       "+------+----+\n",
       "| 1000 |  I |\n",
       "+------+----+"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT COUNT(*) as cnt, op\n",
    "FROM prod_mirror.user_dimension\n",
    "group by op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc3b03-7e3c-4977-a211-8ae0a25f4d2d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Loading incremental data in lake house tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0a62ef1a-7d11-4e72-abee-e58f3468ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mock_dms_data_d.writeTo(\"prod_mirror.user_dimension\").append()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "214e8e3e-3adc-47e8-bf89-e2e4923ce31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>cnt</th>\n",
       "            <th>op</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1000</td>\n",
       "            <td>D</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1000</td>\n",
       "            <td>I</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+------+----+\n",
       "|  cnt | op |\n",
       "+------+----+\n",
       "| 1000 |  D |\n",
       "| 1000 |  I |\n",
       "+------+----+"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT COUNT(*) as cnt, op\n",
    "FROM prod_mirror.user_dimension\n",
    "group by op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8c632010-d3bf-46da-b81d-b993c7353c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mock_dms_data_u.writeTo(\"prod_mirror.user_dimension\").append()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f9fa3d85-8784-4540-a7b2-5bfe43bc642d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>cnt</th>\n",
       "            <th>op</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>3000</td>\n",
       "            <td>U</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1000</td>\n",
       "            <td>I</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1000</td>\n",
       "            <td>D</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+------+----+\n",
       "|  cnt | op |\n",
       "+------+----+\n",
       "| 3000 |  U |\n",
       "| 1000 |  I |\n",
       "| 1000 |  D |\n",
       "+------+----+"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT COUNT(*) as cnt, op\n",
    "FROM prod_mirror.user_dimension\n",
    "group by op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9fddb808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>col_name</th>\n",
       "            <th>data_type</th>\n",
       "            <th>comment</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>name</td>\n",
       "            <td>string</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>address</td>\n",
       "            <td>string</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>license_number</td>\n",
       "            <td>string</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>iban</td>\n",
       "            <td>string</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>bs</td>\n",
       "            <td>string</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>catch_phrase</td>\n",
       "            <td>string</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>company</td>\n",
       "            <td>string</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>paragraph</td>\n",
       "            <td>string</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>created_at</td>\n",
       "            <td>timestamp</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>updated_at</td>\n",
       "            <td>timestamp</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>uuid</td>\n",
       "            <td>string</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Op</td>\n",
       "            <td>string</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>dms_export_timestamp</td>\n",
       "            <td>timestamp</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td></td>\n",
       "            <td></td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td># Metadata Columns</td>\n",
       "            <td></td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>_spec_id</td>\n",
       "            <td>int</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>_partition</td>\n",
       "            <td>struct&lt;&gt;</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>_file</td>\n",
       "            <td>string</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>_pos</td>\n",
       "            <td>bigint</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>_deleted</td>\n",
       "            <td>boolean</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td></td>\n",
       "            <td></td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td># Detailed Table Information</td>\n",
       "            <td></td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Name</td>\n",
       "            <td>demo.prod_mirror.user_dimension</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Type</td>\n",
       "            <td>MANAGED</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Location</td>\n",
       "            <td>s3://warehouse/prod_mirror/user_dimension</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Provider</td>\n",
       "            <td>iceberg</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Owner</td>\n",
       "            <td>root</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Table Properties</td>\n",
       "            <td>[created-at=2024-07-15T11:27:44.510781460Z,current-snapshot-id=7922529467837607368,format=iceberg/parquet,format-version=2,write.format.default=parquet,write.parquet.compression-codec=zstd]</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
       "|                     col_name |                                                                                                                                                                                     data_type | comment |\n",
       "+------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
       "|                         name |                                                                                                                                                                                        string |    None |\n",
       "|                      address |                                                                                                                                                                                        string |    None |\n",
       "|               license_number |                                                                                                                                                                                        string |    None |\n",
       "|                         iban |                                                                                                                                                                                        string |    None |\n",
       "|                           bs |                                                                                                                                                                                        string |    None |\n",
       "|                 catch_phrase |                                                                                                                                                                                        string |    None |\n",
       "|                      company |                                                                                                                                                                                        string |    None |\n",
       "|                    paragraph |                                                                                                                                                                                        string |    None |\n",
       "|                   created_at |                                                                                                                                                                                     timestamp |    None |\n",
       "|                   updated_at |                                                                                                                                                                                     timestamp |    None |\n",
       "|                         uuid |                                                                                                                                                                                        string |    None |\n",
       "|                           Op |                                                                                                                                                                                        string |    None |\n",
       "|         dms_export_timestamp |                                                                                                                                                                                     timestamp |    None |\n",
       "|                              |                                                                                                                                                                                               |         |\n",
       "|           # Metadata Columns |                                                                                                                                                                                               |         |\n",
       "|                     _spec_id |                                                                                                                                                                                           int |         |\n",
       "|                   _partition |                                                                                                                                                                                      struct<> |         |\n",
       "|                        _file |                                                                                                                                                                                        string |         |\n",
       "|                         _pos |                                                                                                                                                                                        bigint |         |\n",
       "|                     _deleted |                                                                                                                                                                                       boolean |         |\n",
       "|                              |                                                                                                                                                                                               |         |\n",
       "| # Detailed Table Information |                                                                                                                                                                                               |         |\n",
       "|                         Name |                                                                                                                                                               demo.prod_mirror.user_dimension |         |\n",
       "|                         Type |                                                                                                                                                                                       MANAGED |         |\n",
       "|                     Location |                                                                                                                                                     s3://warehouse/prod_mirror/user_dimension |         |\n",
       "|                     Provider |                                                                                                                                                                                       iceberg |         |\n",
       "|                        Owner |                                                                                                                                                                                          root |         |\n",
       "|             Table Properties | [created-at=2024-07-15T11:27:44.510781460Z,current-snapshot-id=7922529467837607368,format=iceberg/parquet,format-version=2,write.format.default=parquet,write.parquet.compression-codec=zstd] |         |\n",
       "+------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "DESCRIBE EXTENDED prod_mirror.user_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c3873c61-697c-4d22-9a81-594dcc2de6da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>count(1)</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>3000</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+----------+\n",
       "| count(1) |\n",
       "+----------+\n",
       "|     3000 |\n",
       "+----------+"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "select count(*) from prod_mirror.user_dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d7f145-8a1c-4d37-a129-8e422d610a07",
   "metadata": {},
   "source": [
    "note that this called a [change log table](https://tabular.io/blog/hello-world-of-cdc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e0e2e6-5298-442d-8012-857451c91b92",
   "metadata": {},
   "source": [
    "## Solution 1: Change Log Table\n",
    "In this we keep all incremental change in data as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5a8e490d-1604-43ed-ba80-b0f12d544563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>count(1)</th>\n",
       "            <th>op</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1000</td>\n",
       "            <td>U</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+----------+----+\n",
       "| count(1) | op |\n",
       "+----------+----+\n",
       "|     1000 |  U |\n",
       "+----------+----+"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "WITH windowed_changes AS (\n",
    "    SELECT\n",
    "        op,\n",
    "        uuid,\n",
    "        updated_at,\n",
    "        row_number() OVER (\n",
    "            PARTITION BY uuid\n",
    "            ORDER BY updated_at DESC) AS row_num\n",
    "    FROM prod_mirror.user_dimension\n",
    "),\n",
    "mirror as (\n",
    "    SELECT uuid, updated_at, op\n",
    "    FROM windowed_changes WHERE row_num = 1 AND op != 'D'\n",
    ")\n",
    "select count(*), op from mirror group by op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "44b7f284-b89e-433a-acdf-501feaf92a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>uuid</th>\n",
       "            <th>op</th>\n",
       "            <th>dms_export_timestamp</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>2d762923-2cb9-4489-b8d8-23d18ad1aa8f</td>\n",
       "            <td>U</td>\n",
       "            <td>2024-07-15 13:25:44.160929</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2d762923-2cb9-4489-b8d8-23d18ad1aa8f</td>\n",
       "            <td>D</td>\n",
       "            <td>2024-07-15 13:01:05.162668</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2d762923-2cb9-4489-b8d8-23d18ad1aa8f</td>\n",
       "            <td>U</td>\n",
       "            <td>2024-07-15 13:01:03.921156</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2d762923-2cb9-4489-b8d8-23d18ad1aa8f</td>\n",
       "            <td>U</td>\n",
       "            <td>2024-07-15 13:01:03.921156</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2d762923-2cb9-4489-b8d8-23d18ad1aa8f</td>\n",
       "            <td>I</td>\n",
       "            <td>2024-07-15 13:01:01.919847</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+--------------------------------------+----+----------------------------+\n",
       "|                                 uuid | op |       dms_export_timestamp |\n",
       "+--------------------------------------+----+----------------------------+\n",
       "| 2d762923-2cb9-4489-b8d8-23d18ad1aa8f |  U | 2024-07-15 13:25:44.160929 |\n",
       "| 2d762923-2cb9-4489-b8d8-23d18ad1aa8f |  D | 2024-07-15 13:01:05.162668 |\n",
       "| 2d762923-2cb9-4489-b8d8-23d18ad1aa8f |  U | 2024-07-15 13:01:03.921156 |\n",
       "| 2d762923-2cb9-4489-b8d8-23d18ad1aa8f |  U | 2024-07-15 13:01:03.921156 |\n",
       "| 2d762923-2cb9-4489-b8d8-23d18ad1aa8f |  I | 2024-07-15 13:01:01.919847 |\n",
       "+--------------------------------------+----+----------------------------+"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "select uuid,op, dms_export_timestamp from prod_mirror.user_dimension where uuid ='2d762923-2cb9-4489-b8d8-23d18ad1aa8f'\n",
    "order by dms_export_timestamp desc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209e2d84-7d33-4f35-a106-36d7f8d86443",
   "metadata": {},
   "source": [
    "## Solution 2: View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a250e13-23c6-43cd-b8ac-594341577ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DROP TABLE IF EXISTS prod_mirror.user_dimension_latest_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d843852b-0a46-40c6-881e-4401ebf01906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: %%sql is a cell magic, but the cell body is empty. Did you mean the line magic %sql (single %)?\n"
     ]
    }
   ],
   "source": [
    "%%sql \n",
    "\n",
    "INSERT INTO user_dimension_latest_view\n",
    "SELECT\n",
    "    uuid,\n",
    "    current_date() AS creation_date,\n",
    "    count(1) AS new_accounts\n",
    "FROM prod_mirror.user_dimension a JOIN signups s ON s.acct_id = a.acct_id\n",
    "WHERE a.created_on = current_date()\n",
    "GROUP BY s.branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee38739f-c727-40b7-922d-d09d01f8e446",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('tests/42.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01522aad-ca37-4de6-9c99-3385eaf95a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/20 14:21:49 ERROR Executor: Exception in task 0.0 in stage 4.0 (TID 4)/ 1]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:368)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:368)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2857/0x0000000101365040.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:519)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:391)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2852/0x0000000101360840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2849/0x000000010134fc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1729/0x0000000100c53c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/07/20 14:21:49 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 4.0 (TID 4),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:368)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:368)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2857/0x0000000101365040.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:519)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:391)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2852/0x0000000101360840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2849/0x000000010134fc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1729/0x0000000100c53c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/07/20 14:21:49 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 4) (981a1bf21baf executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:368)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:368)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2857/0x0000000101365040.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:519)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:391)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2852/0x0000000101360840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2849/0x000000010134fc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1729/0x0000000100c53c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/07/20 14:21:49 ERROR TaskSetManager: Task 0 in stage 4.0 failed 1 times; aborting job\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:1261\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1261\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:181\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 181\u001b[0m     converted \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:151\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DateTimeException(origin\u001b[38;5;241m=\u001b[39me)\n\u001b[0;32m--> 151\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mis_instance_of\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.SparkRuntimeException\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkRuntimeException(origin\u001b[38;5;241m=\u001b[39me)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:464\u001b[0m, in \u001b[0;36mis_instance_of\u001b[0;34m(gateway, java_object, java_class)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjava_class must be a string, a JavaClass, or a JavaObject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgateway\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy4j\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreflection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTypeUtil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misInstanceOf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjava_object\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling z:py4j.reflection.TypeUtil.isInstanceOf",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:1261\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m \n\u001b[1;32m   1243\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1261\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcollectToPython()\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/traceback_utils.py:81\u001b[0m, in \u001b[0;36mSCCallSiteSync.__exit__\u001b[0;34m(self, type, value, tb)\u001b[0m\n\u001b[1;32m     79\u001b[0m SCCallSiteSync\u001b[38;5;241m.\u001b[39m_spark_stack_depth \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m SCCallSiteSync\u001b[38;5;241m.\u001b[39m_spark_stack_depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetCallSite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffd2c03",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Schema Evolution\n",
    "\n",
    "Adding, dropping, renaming, or altering columns is easy and safe in Iceberg. In this example, we'll rename `fare_amount` to `fare` and `trip_distance` to `distance`. We'll also add a float column `fare_per_distance_unit` immediately after `distance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efee8252",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "ALTER TABLE nyc.taxis RENAME COLUMN fare_amount TO fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794de3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "ALTER TABLE nyc.taxis RENAME COLUMN trip_distance TO distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adac7564",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "ALTER TABLE nyc.taxis ALTER COLUMN distance COMMENT 'The elapsed trip distance in miles reported by the taximeter.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d7e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "ALTER TABLE nyc.taxis ALTER COLUMN distance TYPE double;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb4b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "ALTER TABLE nyc.taxis ALTER COLUMN distance AFTER fare;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f7cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "ALTER TABLE nyc.taxis\n",
    "ADD COLUMN fare_per_distance_unit float AFTER distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416b498",
   "metadata": {},
   "source": [
    "Let's update the new `fare_per_distance_unit` to equal `fare` divided by `distance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18771ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "UPDATE nyc.taxis\n",
    "SET fare_per_distance_unit = fare/distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c72ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT\n",
    "VendorID\n",
    ",tpep_pickup_datetime\n",
    ",tpep_dropoff_datetime\n",
    ",fare\n",
    ",distance\n",
    ",fare_per_distance_unit\n",
    "FROM nyc.taxis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37582e02",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Expressive SQL for Row Level Changes\n",
    "With Iceberg tables, `DELETE` queries can be used to perform row-level deletes. This is as simple as providing the table name and a `WHERE` predicate. If the filter matches an entire partition of the table, Iceberg will intelligently perform a metadata-only operation where it simply deletes the metadata for that partition.\n",
    "\n",
    "Let's perform a row-level delete for all rows that have a `fare_per_distance_unit` greater than 4 or a `distance` greater than 2. This should leave us with relatively short trips that have a relatively high fare per distance traveled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded820f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DELETE FROM nyc.taxis\n",
    "WHERE fare_per_distance_unit > 4.0 OR distance > 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef3712",
   "metadata": {},
   "source": [
    "There are some fares that have a `null` for `fare_per_distance_unit` due to the distance being `0`. Let's remove those as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b69265",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DELETE FROM nyc.taxis\n",
    "WHERE fare_per_distance_unit is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b92d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT\n",
    "VendorID\n",
    ",tpep_pickup_datetime\n",
    ",tpep_dropoff_datetime\n",
    ",fare\n",
    ",distance\n",
    ",fare_per_distance_unit\n",
    "FROM nyc.taxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5472b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT COUNT(*) as cnt\n",
    "FROM nyc.taxis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b157e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Partitioning\n",
    "\n",
    "A tables partitioning can be updated in place and applied only to newly written data. Query plans are then split, using the old partition scheme for data written before the partition scheme was changed, and using the new partition scheme for data written after. People querying the table dont even have to be aware of this split. Simple predicates in WHERE clauses are automatically converted to partition filters that prune out files with no matches. This is whats referred to in Iceberg as *Hidden Partitioning*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e3e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "ALTER TABLE nyc.taxis\n",
    "ADD PARTITION FIELD VendorID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce6bb4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Metadata Tables\n",
    "\n",
    "Iceberg tables contain very rich metadata that can be easily queried. For example, you can retrieve the manifest list for any snapshot, simply by querying the table's `snapshots` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fade1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT snapshot_id, manifest_list\n",
    "FROM nyc.taxis.snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64887133",
   "metadata": {},
   "source": [
    "The `files` table contains loads of information on data files, including column level statistics such as null counts, lower bounds, and upper bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb712f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT file_path, file_format, record_count, null_value_counts, lower_bounds, upper_bounds\n",
    "FROM nyc.taxis.files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65deb074",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Time Travel\n",
    "\n",
    "The history table lists all snapshots and which parent snapshot they derive from. The `is_current_ancestor` flag let's you know if a snapshot is part of the linear history of the current snapshot of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab64f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT *\n",
    "FROM nyc.taxis.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47129d69",
   "metadata": {},
   "source": [
    "You can time-travel by altering the `current-snapshot-id` property of the table to reference any snapshot in the table's history. Let's revert the table to it's original state by traveling to the very first snapshot ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c360238",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql --var df\n",
    "\n",
    "SELECT *\n",
    "FROM nyc.taxis.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df43d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_snapshot = df.head().snapshot_id\n",
    "spark.sql(f\"CALL system.rollback_to_snapshot('nyc.taxis', {original_snapshot})\")\n",
    "original_snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT\n",
    "VendorID\n",
    ",tpep_pickup_datetime\n",
    ",tpep_dropoff_datetime\n",
    ",fare\n",
    ",distance\n",
    ",fare_per_distance_unit\n",
    "FROM nyc.taxis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b71c76",
   "metadata": {},
   "source": [
    "Another look at the history table shows that the original state of the table has been added as a new entry\n",
    "with the original snapshot ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b801d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT *\n",
    "FROM nyc.taxis.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85667efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT COUNT(*) as cnt\n",
    "FROM nyc.taxis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
